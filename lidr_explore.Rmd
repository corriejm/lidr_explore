---
title: "getting_started"
author: "CM"
date: "2025-05-14"
output: html_document
---
https://r-lidar.github.io/lidRbook/

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Install packages
```{r}
install.packages("lidR")
install.packages("ggplot2")
install.packages("here")
install.packages("RCSF")
install.packages("RMCC")
install.packages("terra")
library(lidR)
library(here)
library(ggplot2)
library(RCSF)
library(RMCC)
library(terra)
```

## read in laz file
can also just read in a portion of the laz file to make it smaller
Examples of other attribute abbreviations are: t - gpstime, a - scan angle, n - number of returns, r - return number, c - classification, s - synthetic flag, k - keypoint flag, w - withheld flag, o - overlap flag (format 6+), u - user data, p - point source ID, e - edge of flight line flag, d - direction of scan flag
Also can filter can select rows (points) during reading process. for ex) just load first returns

data is from https://portal.opentopography.org/lidarOutput?jobId=pc1747248628056
```{r}
here()
las <- readLAS(here("data/points.laz"))
print(las) #print
summary(las)

#select a smaller portion of las file to make it run faster
las_lite <- readLAS(here("data/points.laz"), select = "xyzi") #only x, y, z, and intensity
print(las_lite)

#read in only first returns
las_first <- readLAS(here("data/points.laz"), filter = "-keep_first") # Read only first returns
print(las_first)

#get help for full list of available commands
#readLAS(filter = "-help")
```
## Validating LiDAR data
Ensure data is complete and valid according to ASPRS LAS specifications https://www.asprs.org/wp-content/uploads/2019/07/LAS_1_4_r15.pdf
This is to avoid bugs
Common problems are duplicate points, invalide return numbers, incoherent return number and number of return attributes, invalid coordinate reference systems
ALWAYS RUN las_check()

```{r}
# run check
las_check(las)

#address issues
# create a dup with all points
las_orig <- las
#remove duplicates
las <- filter_duplicates(las)
las_check(las)
```
# Rendering
### Basic 3d rendering
Users can change the attributes used for coloring by providing the name of the attribute used to colorize the points. The background color of the viewer can also be changed by assigning a color using the bg argument. Axes can also be added and point sizes can be changed. Note that if your file contains RGB data the string "RGB" is supported
```{r}
plot(las)

# Plot las object by scan angle, 
# make the background white, 
# display XYZ axis and  scale colors
plot(las, color = "ScanAngleRank", bg = "white", axis = TRUE, legend = TRUE)

#RGB
#plot(las, color="RGB")

#Breaks argument
plot(las, color = "Intensity", breaks = "quantile", bg = "white")
```

### Overlays
Easy to use functions for common overlay. For example add_dtm3d() to add a digital terrain model (section Chapter 4)) and add_treetops3d() to visualize the output of an individual tree detection (section Section 7.1))
It is also possible to combine two point clouds with different color palettes. In the following example we are using a previously classified point cloud. We first separate the vegetation and non vegetation points using filter_poi() and then plot both on top of each other with different colour schemes using add options in plot()
```{r}
#point cloud on top of dtm
# x <- plot(las, bg = "white", size = 3)
# add_dtm3d(x, dtm)

#point cloud with individual tree detection
# x <- plot(las, bg = "white", size = 3)
# add_treetops3d(x, ttops)

#two point clouds
# nonveg <- filter_poi(las, Classification != LASHIGHVEGETATION)
# veg <- filter_poi(las, Classification == LASHIGHVEGETATION)
# 
# x <- plot(nonveg, color = "Classification", bg = "white", size = 3)
# plot(veg, add = x)
```

### Advanced 3d rendering
Since lidR is based on rgl, it is easy to add objects to the main rendering using rgl functions such as rgl::point3d(), rgl::text(), rgl::surface3d(), and so on to produce publication-ready renderings. However, lidR introduces an additional challenge: it does not display the points with their actual coordinates. Instead, the points are shifted to be rendered close to (0, 0) due to accuracy issues, as rgl uses float (32-bit decimal numbers) rather than double (64-bit decimal numbers). When plot() is used, it invisibly returns the shift values, which can later be used to realign other objects.
The coordinates of the objects must be corrected to align with the point cloud. In the following we will add lines to render the trunks. We read a file, we locate the trees (see Section 7.1)), we extract the coordinates and sizes of the trees and plot lines with rgl::segment3d().

```{r}
# offsets <- plot(las)
# print(offsets)
# #> [1]  391867.8 3901019.3
# 
# LASfile <- system.file("extdata", "MixedConifer.laz", package="lidR")
# las <- readLAS(LASfile, select = "xyzc")
# 
# # get the location of the trees
# ttops <- locate_trees(las, lmf(ws = 5)) 
# 
# # plot the point cloud
# offsets <- plot(las, bg = "white", size = 3)
# add_treetops3d(offsets, ttops)
# 
# # extract the coordinates of the trees and
# # apply the shift to display the lines
# # in the rendering coordinate system
# x <- sf::st_coordinates(ttops)[,1] - offsets[1] 
# y <- sf::st_coordinates(ttops)[,2] - offsets[2] 
# z <- ttops$Z
# 
# # Build a GL_LINES matrix for fast rendering
# x <- rep(x, each = 2)
# y <- rep(y, each = 2)
# tmp <- numeric(2*length(z)) 
# tmp[2*1:length(z)] <- z
# z <- tmp
# M <- cbind(x,y,z)
# 
# # Display lines
# rgl::segments3d(M, col = "black", lwd = 2)
```
### Voxel rendering 
It is possible to render voxels. This is useful to render the output of the function voxelise_points() or voxel_metrics() for examples.
```{r}
# vox <- voxelize_points(las, 6)
# plot(vox, voxel = TRUE, bg = "white")

```
### Cross SEctions 2d Rendering
To better visualize the vertical structure of a point cloud, investigate classification results, or compare the results of different interpolation routines, a cross section can be plotted. To do this, we first need to decide where the cross section will be located (i.e., define the beginning and end) and specify its width. The point cloud can then be clipped, and the X and Z coordinates used to create the plot.
For example, to create a 200 m long cross section, we might define the beginning and end, and then use the clip_transect() function to subset the point cloud.
Rendering can be achieved with base plot or ggplot2. Notice the use of payload() to extract the data.frame from the LAS object.
```{r}
#extent       : 642950.1, 643771.5, 4445374, 4446583 (xmin, xmax, ymin, ymax)

p1 <- c(642950, 4445374)
p2 <- c(643300, 4445400)
las_tr <- clip_transect(las, p1, p2, width = 5, xz = TRUE)

ggplot(payload(las_tr), aes(X,Z, color = Z)) +
  geom_point(size = 0.5) +
  coord_equal() +
  theme_minimal() +
  scale_color_gradientn(colours = height.colors(50))
```

#Ground Classification
Classification of ground points is an important step in processing point cloud data. Distinguishing between ground and non-ground points allows creation of a continuous model of terrain elevation (see Chapter 4)). Many algorithms have been reported in the literature and lidR currently provides three of them: Progressive Morphological Filter (PMF), Cloth Simulation Function (CSF) and Multiscale Curvature Classification (MCC) usable with the function classify_ground().

How to Choose
(From chatgpt: use PMF for quick results on flat areas, CSF for natural terrain, mountains or forested areas, MCC when dealing with urban areas of mixed natural/built environments)

Identifying the optimal algorithm parameters is not a trivial task and often requires several trial runs. lidR proposes several algorithms and may introduce even more in future versions; however, the main goal is to provide a means to compare outputs. We don’t know which one is better or which parameters best suit a given terrain. It’s likely that parameters need to be dynamically adjusted to the local context, as parameters that work well in one file may yield inadequate results in another.

Note
lidR is an research and development package, not a production package. The goal of lidR is to provide numerous and handy ways to manipulate, test, compare point cloud processing methods. If available, we recommend using classifications provided by the data provider. The classify_ground() function is useful for small to medium-sized unclassified regions of interest because it is feasible to visually assess classification results. For large acquisitions where visual assessment is no longer feasible, we do not recommend performing ground classification without first studying its accuracy.

## PMF Progressive Morphological Filter
The implementation of PMF algorithm in lidR is based on the method described in Zhang et al. (2003) with some technical modifications. The original method is raster-based, while lidR performs point-based morphological operations because lidR is a point cloud oriented software. Iterative process of comparing and removing points based on a threshold, make ground points. then open point cloud with a bigger window and repeat.
The pmf() function requires defining the following input parameters: ws (window size or sequence of window sizes), and th (threshold size or sequence of threshold heights). More experienced users may experiment with these parameters to achieve best classification accuracy, however lidR contains util_makeZhangParam() function that includes the default parameter values described in Zhang et al. (2003).

```{r}
#pmf
LASfile <- system.file("extdata", "Topography.laz", package="lidR") #this is from the example file in lidr package
las_pmf_ex <- readLAS(LASfile, select = "xyzrn") # creates
las_pmf1 <- classify_ground(las_pmf_ex, algorithm = pmf(ws = 5, th = 3)) # ws is window size in meters, th is elevation threshold, points differing more than 3 meters are not considered ground

#visualize
plot(las_pmf1, color = "Classification", size = 3, bg = "white") 


#this didnt work that well so need to adjust the ws and the th parameter
#pmf adjusted
ws <- seq(3, 12, 3)
th <- seq(0.1, 1.5, length.out = length(ws))
las_pmf2 <- classify_ground(las_pmf_ex, algorithm = pmf(ws = ws, th = th))


#visualize
plot(las_pmf2, color = "Classification", size = 3, bg = "white") 

#try with a clipped version of my dataset
las_clipped <- clip_circle(las, 643300, 4445400, 100)
las_clipped
las_pmf<- classify_ground(las_clipped, algorithm = pmf(ws = ws, th = th))
#visualize
plot(las_pmf, color = "Classification", size = 3, bg = "white") 
#cross section
p10 <- c(643200, 4445374)
p20 <- c(643400, 4445500)
las_pmf2_tr <- clip_transect(las_pmf, p10, p20, width = 5, xz = TRUE)

ggplot(payload(las_pmf2_tr), aes(X, Z, color = factor(Classification))) +
  geom_point(size = 0.5) +
  coord_equal() +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red", "green"))
```
## Cloth Simulation Function
Cloth simulation filtering (CSF) uses the Zhang et al 2016 algorithm and consists of simulating a piece of cloth draped over a reversed point cloud. In this method the point cloud is turned upside down and then a cloth is dropped on the inverted surface. Ground points are determined by analyzing the interactions between the nodes of the cloth and the inverted surface. The cloth simulation itself is based on a grid that consists of particles with mass and interconnections that together determine the three-dimensional position and shape of the cloth.
The csf() functions use the default values proposed by Zhang et al 2016 and can be used without providing any arguments.

While the default parameters of csf() are designed to be universal and provide accurate classification results, according to the original paper, it’s apparent that the algorithm did not work properly in our example because a significant portion of points located in the ground were not classified. In such cases the algorithm parameters need to be tuned to improve the result. For this particular data set a set of parameters that resulted in an improved classification result were formulated as follows:
```{r}
las_csf <- classify_ground(las_pmf_ex, algorithm = csf())
#visualize
plot(las_csf, color = "Classification", size = 3, bg = "white")

#with my clipped dataset
las_csf <- classify_ground(las_clipped, algorithm = csf())
#visualize
plot(las_csf, color = "Classification", size = 3, bg = "white") 
#cross section
p10 <- c(643200, 4445374)
p20 <- c(643400, 4445500)
las_csf_tr <- clip_transect(las_csf, p10, p20, width = 5, xz = TRUE)

ggplot(payload(las_csf_tr), aes(X, Z, color = factor(Classification))) +
  geom_point(size = 0.5) +
  coord_equal() +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red", "green"))

# try modified csf parameters
mycsf <- csf(sloop_smooth = TRUE, class_threshold = 1, cloth_resolution = 1, time_step = 1)
las_csf_m <- classify_ground(las_clipped, mycsf)
#visualize
#visualize
plot(las_csf_m, color = "Classification", size = 3, bg = "white") 
#cross section
p10 <- c(643200, 4445374)
p20 <- c(643400, 4445500)
las_csfm_tr <- clip_transect(las_csf_m, p10, p20, width = 5, xz = TRUE)

ggplot(payload(las_csfm_tr), aes(X, Z, color = factor(Classification))) +
  geom_point(size = 0.5) +
  coord_equal() +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red", "green"))

#compare with provided classification

las_clipped_tr <- clip_transect(las_clipped, p10, p20, width = 5, xz = TRUE)
ggplot(payload(las_clipped), aes(X, Z, color = factor(Classification))) +
  geom_point(size = 0.5) +
  coord_equal() +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red", "green", "purple"))
#3d visualization
# gnd <- filter_ground(las_csf_m)
# plot(gnd, size = 3, bg = "white") 
```

## Multiscale Curvature Classification MCC
Multiscale Curvature Classification (MCC) uses the Evans and Hudak 2016 algorithm originally implemented in the mcc-lidar software.

```{r}
las_mcc_ex <- classify_ground(las_pmf_ex, mcc(1.5,0.3))

#visualize
las_mcc_tr <- clip_transect(las_mcc_ex, p10, p20, width = 5, xz = TRUE)
#3d visualization
gnd <- filter_ground(las_mcc_ex)
plot(gnd, size = 3, bg = "white")
```

## Edge Artifacts
No matter which algorithm is used in lidR or other software, ground classification will be weaker at the edges of point clouds as the algorithm must analyze the local neighbourhood (which is missing on edges). To find ground points, an algorithm need to analyze the local neighborhood or local context that is missing at edge areas. When processing point clouds it’s important to always consider a buffer around the region of interest to avoid edge artifacts. lidR has tools to manage buffered tiles and this advanced use of the package will be covered in Chapter 14).

# Digital Terrain Models
Generating a Digital Terrain Model (DTM) is usually the second step in processing that follows classification of ground points (Chapter 4). Put simply, a DTM can be described as an “image” of the ground. Methods to generate DTMs have been intensively studied and several algorithms have been proposed for various terrain situations. DTMs are used for a variety of purposes in practice, such as determination of the catchment basins of water retention and stream flow, or the identification of drivable roads to access resources. It also enables users to normalize point clouds i.e. subtract the local terrain from the elevation of points to allow a manipulation of point clouds as if they were acquired on a flat surface (Chapter 6).

The construction of a DTM starts with known or sampled ground points and uses various spatial interpolation techniques to infer ground points at unsampled locations. Accuracy of the DTM is very important because errors will propagate to future processing stages like tree height estimation. A wide range of methods exist for spatial interpolation of points. In the following section we will use the classified Topography.laz data set, which is included internally within lidR to create reproducible examples.

Triangulation is a very fast and efficient method that generates very good DTMs and is robust to empty regions inside the point cloud. It is however weak at edges. Although lidR uses the nearest neighbour to complete the missing pixel out of the convex hull of the ground points the interpolation remains poor. This algorithm must therefore always be used with a buffer of extra points to ensure that the region of interest is not on an edge. The TIN method is recommended for broad DTM computation but should be avoided for small regions of interest loaded without buffers.

Invert distance weighting is fast, but approximately twice as slower than TIN. The terrain is not very realistic, but edges are likely to be free of strong edge artifacts. IDW is a compromise between TIN and KRIGING. It is recommended if you want a simple method, if you cannot load a buffer, and if edge regions are important.

Kriging is very slow because it is computationally demanding. It is not recommended for use on medium to large areas. It can be used for small plots without buffers to get a nice DTM without strong edges artifact.

Whatever the method used, edges are critical. Results will always be weak if the method needs to guess the local topography with only partial information on the neighborhood. Though different methods provide better and worse estimates in these regions, best practice is to always use a buffer to obtain some information about the neighborhood and remove the buffer once the terrain is computed.
```{r}
#call in from lidR package
LASfile <- system.file("extdata", "Topography.laz", package="lidR")
las_dtm <- readLAS(LASfile, select = "xyzc")
plot(las_dtm, size = 3, bg = "white")
```
## TIN triangular irregular network
This method is based on triangular irregular network (TIN) of ground point data to derive a bivariate function for each triangle, which is then used to estimate the values at unsampled locations (between known ground points).
Planar facets of each generated triangle are used to interpolate. Used with a Delaunay triangulation, this is the most simple solution because it involves no parameters. The Delaunay triangulation is unique and the linear interpolation is parameter-free. The drawbacks of the method are that it creates a non-smooth DTM and that it cannot extrapolate the terrain outside the convex hull delimited by the ground points since there are no triangle facets outside the convex hull. Moreover, the interpolation is weak at the edges because large irrelevant triangles are often created. It’s therefore important to compute the triangulation with a buffer to be able to crop the DTM and clear the edge artifacts (see Chapter 15).
To generate a DTM model with the TIN algorithm we use rasterize_terrain() where algorithm = tin(). Notice the ugly edge interpolations. This occurs because we didn’t process with a buffer.
```{r}
dtm_tin <- rasterize_terrain(las_dtm, res = 1, algorithm = tin())
plot_dtm3d(dtm_tin, bg = "white") 

```

## Inverse Distance Weighting
Invert distance weighting (IDW) is one of the simplest and most readily available methods that can be applied to create DTMs. It is based on an assumption that the value at an unsampled point can be approximated as a weighted average of values at points within a certain cut-off distance d, or from a given number k of closest neighbours. Weights are usually inversely proportional to a power p of the distance between the location and the neighbour, which leads to the computing of an estimator.

Compared to tin() this method is more robust to edge artifacts because it uses a more relevant neighbourhood but generates terrains that are “bumpy” and probably not as realistic as those generated using TINs. There are always trade-offs to different methods!

To generate a DTM model with the IDW algorithm we use rasterize_terrain() where algorithm = knnidw().
Notice the bumpy nature of the DTM compared to the previous one generated with tin(). In 1D and IDW interpolation looks like:
```{r}
dtm_idw <- rasterize_terrain(las_dtm, algorithm = knnidw(k = 10L, p = 2))
plot_dtm3d(dtm_idw, bg = "white") 
```

## Kriging
Kriging is the most advanced approach and utilizes advanced geostatistical interpolation methods that take into account the relationships between the returns and their respective distances from each other. lidR uses the package gstat to perform the kriging. This method is very advanced, difficult to manipulate, and extremely slow to compute, but probably provides the best results with minimal edge artifacts.

To generate a DTM model with the kriging algorithm we use rasterize_terrain() where algorithm = kriging().
```{r}
#takes a long time
dtm_kriging <- rasterize_terrain(las_dtm, algorithm = kriging(k = 40))
plot_dtm3d(dtm_kriging, bg = "white")
```

## Render a shadowed DTM aka Hillshade
```{r}
dtm <- rasterize_terrain(las_dtm, algorithm = tin())
dtm_prod <- terrain(dtm, v = c("slope", "aspect"), unit = "radians")
dtm_hillshade <- shade(slope = dtm_prod$slope, aspect = dtm_prod$aspect)
plot(dtm_hillshade, col =gray(0:30/30), legend = FALSE)

```